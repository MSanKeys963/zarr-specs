.. _sharding-storage-transformer-v1:

==========================================
Sharding storage transformer (version 1.0)
==========================================
-----------------------------
 Editor's draft 18 02 2022
-----------------------------

Specification URI:
    @@TODO
    http://purl.org/zarr/spec/storage_transformers/sharding/1.0
Issue tracking:
    `GitHub issues <https://github.com/zarr-developers/zarr-specs/labels/storage_transformers-sharding-v1.0>`_
Suggest an edit for this spec:
    `GitHub editor <https://github.com/zarr-developers/zarr-specs/blob/core-protocol-v3.0-dev/docs/storage_transformers/sharding/v1.0.rst>`_

Copyright 2022 `Zarr core development
team <https://github.com/orgs/zarr-developers/teams/core-devs>`_ (@@TODO
list institutions?). This work is licensed under a `Creative Commons
Attribution 3.0 Unported
License <https://creativecommons.org/licenses/by/3.0/>`_.

----


Abstract
========

This specification defines an implementation of the Zarr
storage transformer protocol for sharding.


Motivation
==========

Sharding decouples the concept of chunks from storage keys, which become shards.
This is helpful when the requirements for those don't align:

- Compressible units of chunks often need to be read and written in smaller
  chunks, whereas
- storage often is optimized for larger data per entry and fewer entries, e.g.
  as restricted by the file block size and maximum inode number for typical
  file systems.

This does not necessarily fit the access patterns of the data, so chunks might
need to be smaller than one storage key. In those cases sharding decouples those
entities. One shard corresponds to one storage key, but can contain multiple chunks:

.. image:: sharding.png


Document conventions
====================

Conformance requirements are expressed with a combination of
descriptive assertions and [RFC2119]_ terminology. The key words
"MUST", "MUST NOT", "REQUIRED", "SHALL", "SHALL NOT", "SHOULD",
"SHOULD NOT", "RECOMMENDED", "MAY", and "OPTIONAL" in the normative
parts of this document are to be interpreted as described in
[RFC2119]_. However, for readability, these words do not appear in all
uppercase letters in this specification.

All of the text of this specification is normative except sections
explicitly marked as non-normative, examples, and notes. Examples in
this specification are introduced with the words "for example".


Configuration
=============

Sharding can be configured per array in the :ref:`array-metadata`:

.. code-block::

    {
      storage_transformers: [
        {
          "storage_transformer": "https://purl.org/zarr/spec/storage_transformers/sharding/1.0",
          "configuration": {
            "format": "indexed",
            "chunks_per_shard": [
                2,
                2
            ]
          }
      ]
    }

``format``

    Specifies a `Binary shard format`_. In this version, the only binary format is the
    ``indexed`` format.

``chunks_per_shard``

    An array of integers providing the number of chunks that are combined in a shard
    for each dimension of the Zarr array, where each chunk may only start at a position
    that is divisble by ``chunks_per_shard`` per dimension, e.g. starting at the zero-origin.
    The length of the array must match the length of the array metadata ``shape`` entry.
    For example, a value ``[32, 2]`` indicates that 64 chunks are combined in one shard,
    32 along the first dimension, and for each of those 2 along the second dimension.
    Valid starting positions for a shard in the chunk-grid are therefore ``[0, 0]``,
    ``[32, 2]``, ``[32, 4]``, ``[64, 2]`` or ``[96, 18]``.


Key & value transformation
==========================

The storage transformer protocol defines the abstract interface to be the same
as the `Abstract store interface`_.

The Zarr store interface is defined in terms of `keys` and `values`,
where a `key` is a sequence of characters and a `value` is a sequence
of bytes. A key-value pair is called entry in the following part.

This sharding transformer only adapts entries where the key starts
with `data/root`, as they indicate data keys for array chunks. All other
entries are simply passed on.

Entries starting with `data/root` are grouped by their common shard, assuming
`Storage keys` from a regular chunk grid which may use a customly configured
``chunk separator``:
For all entries that are part of the same shard the key is changed to the
shard-key and the values are combined in the `Binary shard format`_ described
below. The new shard-key is the chunk key divided by ``chunks_per_shard`` and
floored per dimension. E.g. for ``chunks_per_shard=[32, 2]``, the chunk grid
position ``[96, 18]`` (e.g. key "data/root/foo/baz/c96/18") is transformed to
the shard grid position ``[3, 9]`` and reassigned to the respective new key,
honoring the original chunk separator (e.g. "data/root/foo/baz/c3/9").
Chunk grid positions ``[96, 19]``, ``[97, 18]``, â€¦, up to ``[127, 19]`` will
also have the same shard grid position ``[3, 9]``.


Binary shard format
===================

The only binary format is the ``indexed`` format, as specified by the ``format``
configuration key. Other binary formats might be added in future versions.

In the indexed binary format chunks are written successively in a shard, where
unused space between them is allowed, followed by an index referencing them.
The index holds an `offset, length` pair of little-endian uint64 per chunk,
the chunks-order in the index is row-major (C) order, e.g. for (2, 2) chunks
per shard an index would look like:

.. code-block::

    | chunk (0, 0)    | chunk (0, 1)    | chunk (1, 0)    | chunk (1, 1)    |
    | offset | length | offset | length | offset | length | offset | length |
    | uint64 | uint64 | uint64 | uint64 | uint64 | uint64 | uint64 | uint64 |


Empty chunks are denoted by setting both offset and length to `2^64 - 1``.
The index always has the full shape of all possible chunks per shard,
even if they are outside of the array size.

The actual order of the chunk-content is not fixed and may be chosen by the implementation
as all possible write orders are valid according to this specification and therefore can
be read by any other implementation. When writing partial chunks into an existing shard no
specific order of the existing chunks may be expected. Some writing strategies might be

* **Fixed order**: Specify a fixed order (e.g. row-, column-major or Morton order).
  When replacing existing chunks larger or equal sized chunks may be replaced in-place,
  leaving unused space up to an upper limit  which might possibly be specified.
  Please note that for regular-sized uncompressed data all chunks have the same size and
  can therefore be replaced in-place.
* **Append-only**: Any chunk to write is appended to the existing shard, followed by an updated index.

Any configuration parameters for the write strategy must not be part of the metadata document,
in a shard I'd propose to use Morton order, but this can easily be changed and customized, since any order can be read.


References
==========

.. [RFC2119] S. Bradner. Key words for use in RFCs to Indicate
   Requirement Levels. March 1997. Best Current Practice. URL:
   https://tools.ietf.org/html/rfc2119


Change log
==========

@@TODO
